Q5(Frozen 갱신): 영구 고정은 비추. 다만 “자주 갱신”도 비추. → 버전 이력 유지 + 드리프트(괴리) 기준으로 가끔 갱신이 가장 안정적.

Q6(기대 점수): 필수는 아님. 하지만 “상대 비교(A)만” 하면 품질이 특정 방향으로 치우칠 수 있어. → 실무에선 A(상대승격) + 최소 절대 기준(게이트) + 일부 골든샘플에만 B(범위) 조합이 제일 잘 굴러감.

Q5. Evaluator_Frozen은 언제 갱신?
1) Frozen은 “영구”가 아니라 “기준선 베이스라인”으로 운영

Frozen의 역할은 변수 통제된 비교 기준이야.

따라서 갱신하더라도 “덮어쓰기”가 아니라 Frozen_v1, Frozen_v2…로 누적해야 검증 의미가 살아.

2) 갱신 방식 추천: “주기”보다 “조건(드리프트/변경)” 기반

주기 갱신(매달 등)도 가능하지만, 보통은 아래 조건이 발생할 때만 갱신하는 게 좋아.

갱신 트리거(추천)

(필수) 평가 기준/루브릭이 변경됨 (가중치, 항목 정의, 제출 형식 변경 등)

(필수) 모델/시스템 메시지/온도 등 평가 엔진 자체가 바뀜

(권장) Live와 Frozen의 괴리가 커져서 “검증이 현실을 못 따라감”

예: 같은 Suite에서 Frozen vs Live 평균 점수 차이가 지속적으로 5~8점 이상

또는 핵심 결함 태그 분포가 완전히 달라짐(예: Live는 FORMAT_NONCOMPLIANCE를 거의 안 잡는데 Frozen은 계속 잡음)

3) 갱신 주기 권장안(현업에서 무난한 기본값)

분기(3개월) 단위 점검은 하되, 실제 갱신은 “필요할 때만”

즉,

매달 “점검” (드리프트 체크)

갱신은 “분기 1회 이하” 수준이 보통 안정적

4) 갱신 절차(중요: 비교 연속성 유지)

Frozen을 바꿀 때는 이렇게 해야 “검증 기준이 바뀌어 점수가 달라진 것”을 구분할 수 있어.

Frozen 후보(Evaluator_Live를 정리해 새 Frozen으로) 생성

Suite 전체를 Old Frozen과 New Frozen으로 둘 다 재평가

차이(점수/태그) 리포트 저장

운영 규칙:

“승격 비교”는 그 시점의 Frozen 하나로만 수행

과거 추세를 볼 때는 “당시 Frozen 버전 기준”으로 회고

추천 운영 팁: Frozen을 2개 유지하면 좋아.

Frozen_Current(승격 기준)

Frozen_Previous(드리프트/추세 비교용)

Q6. Suite 샘플의 “기대 점수(정답)”가 필요?
결론: 필수는 아니고, 대부분은 **A(상대 비교)**로도 충분함

단, A만 쓰면 “좋은 글”이 아니라 “Frozen이 좋아하는 글”로 수렴할 위험이 있어. 그래서 **절대 게이트 + 일부 B(범위)**를 얹는 걸 추천.

A) 기대 점수 없이 “Champion 대비 상승”만으로 승격

장점

구축이 가장 쉽고 빠름

Frozen이 고정이면(=채점 기준이 고정이면) 상대 비교의 신뢰도가 높아짐

단점

“상대적으로만 좋아짐”이라서, 절대 품질이 낮은 상태에서 점수 장난처럼 보일 수 있음

특정 스타일/패턴에 과적합(점수는 오르는데 사람이 보기엔 별로)

→ 그래서 A를 쓰더라도 **절대 기준(게이트)**는 거의 필수로 붙여.

추천: A + “절대 게이트(필수 통과 조건)”

Suite마다 기대 점수를 만들지 않아도, 아래 게이트만 있으면 품질이 안정적으로 올라가.

절대 게이트 예시

총점 최소: 예) total_score >= 75

핵심 항목 최소: 예) 구체성 >= 15/20 AND 완성도 >= 16/20

치명 태그 0개: 예) MISSING_REQUIRED_ITEM 발생 시 자동 실패

태그 감소 조건: 예) NO_EVIDENCE_OR_CITATION 빈도 감소 필수

이러면 “Champion보다 1점 높아서 승격” 같은 허수를 막아줘.

B) 샘플마다 기대 점수 “범위”를 정해두고 검증

장점

사람 기준 품질을 더 잘 반영

점수 인플레/과적합 감지에 강함

단점

기대 점수 범위를 어떻게 정하느냐가 어려움(라벨링 비용)

섹션/상황이 조금만 달라져도 범위 설정이 애매해질 수 있음

실무 최적 조합(권장)

Suite 전체 승격은 A + 게이트로 처리

Suite 중 일부(예: 2~4개)는 “Golden 샘플”로 지정해서 B(범위) 적용

Golden 샘플의 기대 범위는 어떻게 만드나?

가장 현실적인 방법 3가지:

사람(너/팀)이 1회만 라벨링해서 범위를 잡기 (예: 80~90)

서로 다른 Evaluator 2~3개(모델/프롬프트)를 돌려 합의 평균±허용치로 범위 설정

점수 대신 **체크리스트 기반 규칙 검증(비LLM)**으로 “통과/실패”만 고정
(예: 필수항목 포함 여부, 표 존재 여부, 출처 표기 규칙 준수 등)

특히 3)은 비용 대비 효과가 좋아.
“형식/누락/출처” 같은 건 룰 기반 검증이 LLM보다 안정적이거든.

추천 운영안 요약

Q5: Frozen은 영구 고정 X → 버전 누적 + 드리프트/루브릭/모델 변경 시에만 갱신(보통 분기 이하)

Q6: 기대 점수는 필수 X → 기본은 A(상대) + 절대 게이트

안정화되면 Golden 일부에만 B(범위) 추가