Q1. ChatGPT - Tab3 에서 진행해, 테스트 진행해(질문-답변)
 Tab1: 심층 리서치, Tab2: Rai-사업계획서, Tab3: 프롬프트 개선

Q2. defect_tags는 고정 목록인가요, Evaluator가 자유롭게 생성하나요?
권장: 고정 목록 + 확장 가능(승인 절차)

Evaluator는 기본적으로 “정의된 태그 목록에서만 선택”

단, 실제 운영하다 보면 “새 유형의 결함”이 반드시 생김
→ 그래서 자유생성은 허용하되, 즉시 정식 태그가 되지 않게 해야 추적이 깨지지 않아.

추천 운영 방식(가장 안정적)

Core Tags(고정)

너가 적은 6개는 Core로 고정:
MISSING_REQUIRED_ITEM, NO_EVIDENCE_OR_CITATION, VAGUE_CLAIMS, FORMAT_NONCOMPLIANCE, LOGIC_FLOW_WEAK, DIFFERENTIATION_WEAK

Proposed Tags(후보)

Evaluator가 Core로 커버 안 되는 결함을 발견하면

proposed_tags: ["COMPETITOR_EVIDENCE_WEAK", "METRIC_INCONSISTENT"] 처럼 “제안”만 가능

단, 반드시 Core 중 하나에 임시 매핑도 같이 제출
(예: METRIC_INCONSISTENT → NO_EVIDENCE_OR_CITATION 또는 LOGIC_FLOW_WEAK)

Tag Governance(승인/반려)

Prompt Critic/Builder 단계에서

“반복 발생(예: 3회/2섹션 이상)” + “Core로 구분이 어렵다”면 → 정식 태그 승격

아니면 반려/병합(기존 Core로 흡수)

태그 정의도 같이 고정해줘야 함(중요)

태그는 이름만 고정하면 의미가 흐려져서, 반드시 다음 메타를 붙이는 게 좋아:

tag_id, definition, examples, counter_examples, severity(1~3), auto_fix_hint

결론: Evaluator 자유생성(무제한)은 비추,
Core 고정 + 후보 제안 + 승격 절차가 추적/분석에 제일 유리.

Q3. Champion/Challenger 테스트 샘플은 어디서 확보하나요?
권장: “Evaluation Suite(검증 스위트)”를 별도 자산으로 만든다

샘플은 회차마다 즉흥적으로 뽑으면 편향/누락이 생겨서, 아래처럼 “고정 세트 + 확장” 구조가 안정적이야.

샘플 구성 우선순위(실무적으로 잘 됨)

(1) A: 같은 섹션의 이전 초안들 (강추)

장점: 동일 과제에서 “프롬프트 변경의 효과”가 가장 잘 드러남

주의: 이전 초안이 “나쁜 예”만 있으면, 새 프롬프트가 나쁜 예에만 최적화될 수 있음
→ 상/중/하 점수 초안을 균형있게 포함

(2) B: 유사 성격의 다른 섹션 (강추)

예: 1-2(배경/필요성) ↔ 1-3(목표시장/문제정의)처럼 “근거/논리”가 중요한 섹션끼리

장점: “한 섹션 맞춤 과적합”을 방지

(3) C: 별도 테스트 케이스(가능하면 추천)

목적: 특정 결함을 일부러 포함한 “단위 테스트”

예: 출처가 없는 수치만 있는 입력, 필수항목 1개 누락된 입력, 표 형식이 요구되는 입력 등

장점: 프롬프트 패치가 “의도한 결함”을 진짜 막는지 빠르게 검증

현실적인 최소 세트(바로 운영 가능)

Suite-5 (최소)

A에서 3개(상/중/하) + B에서 2개

Suite-12 (권장)

A 6개(2섹션×상/중/하) + B 4개 + C 2개(단위테스트)

아주 중요한 안정화 팁

**테스트에는 “평가 프롬프트(=Evaluator)를 고정”**해 둬야 함
(Writer를 바꿨는데 Evaluator도 바뀌면, 점수 변화가 프롬프트 개선 때문인지 평가 변화 때문인지 분리 불가)

더 안정적으로 하려면:

Evaluator_Frozen(검증용 고정) + Evaluator_Live(운영용) 2개를 둔다

결론: 샘플은 A/B/C를 섞되, “스위트”로 고정 관리하고, 검증용 Evaluator는 동결하는 게 핵심.

Q4. 개선 루프는 언제 실행? 매 회차 vs N회 누적 후?
권장: 매 회차 “측정/로그”는 하고, 프롬프트 “변경”은 트리거/배치로 제한

즉, 아래처럼 2단으로 나누는 게 제일 안정적이야.

4-1) Online Loop(매 회차) — 프롬프트는 대체로 안 바꿈

매 회차마다:

Writer로 작성

Evaluator로 점수/태그 산출

로그 적재 + defect 카운터 업데이트

(필요 시) 같은 프롬프트로 “텍스트 수정”만 반복

여기서 포인트는:

“이번 회차 결과물 개선”은 매번 해도 됨

“프롬프트 버전업(Writer/Evaluator 수정)”은 매번 하지 않는 게 좋음
(너무 자주 바꾸면 수렴이 아니라 흔들림)

4-2) Prompt Update Loop(트리거/배치) — Critic/Builder가 여기서만 본격 실행

아래 트리거가 걸릴 때만 Critic/Builder 호출:

트리거 예시(문서에 있는 것 확장)

동일 defect_tag가 2회 연속

특정 항목 점수가 임계치 이하가 2회 연속

총점이 3회 정체(개선폭 미미)

새 proposed_tag가 3회 이상 반복(정식 태그 승격 검토)

운영 형태 2가지 중 추천

(추천) 미니 배치: “섹션 단위 완료 시” + “트리거 발생 시”

장점: 섹션 내 노이즈를 줄이고, 개선이 목적에 맞게 반영됨

(대안) 정기 배치: 하루 1회/주 2회 등

장점: 운영 안정적, 변경 관리가 쉬움

실전에서 가장 잘 먹히는 기본값(추천 세팅)

매 회차: Writer/Evaluator 실행 + 태그 카운팅(항상)

Critic 호출: 트리거 발생 시에만(예: 동일 태그 2연속)

Builder로 v+1 생성: Critic에서 “패치 1~2개”만 만들기

승격: Challenger가 Suite-5에서 평균 점수↑ + 핵심 태그↓이면 승격

결론: “매 회차 후 Critic”은 비용도 크고 흔들림도 커서 비추.
매 회차는 관측/로그, 개선(프롬프트 버전업)은 트리거/배치로 제한이 가장 안정적.